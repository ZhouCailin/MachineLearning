{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "lab_gene_partial.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seTCB8BGbkPD"
      },
      "source": [
        "# Lab:  Logistic Regression for Gene Expression Data\n",
        "\n",
        "In this lab, we use logistic regression to predict biological characteristics (\"phenotypes\") from gene expression data.  In addition to the concepts in [breast cancer demo](./breast_cancer.ipynb), you will learn to:\n",
        "* Handle missing data\n",
        "* Perform multi-class logistic classification\n",
        "* Create a confusion matrix\n",
        "* Use L1-regularization for improved estimation in the case of sparse weights (Grad students only)\n",
        "\n",
        "## Background\n",
        "\n",
        "Genes are the basic unit in the DNA and encode blueprints for proteins.  When proteins are synthesized from a gene, the gene is said to \"express\".  Micro-arrays are devices that measure the expression levels of large numbers of genes in parallel.  By finding correlations between expression levels and phenotypes, scientists can identify possible genetic markers for biological characteristics.\n",
        "\n",
        "The data in this lab comes from:\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression\n",
        "\n",
        "In this data, mice were characterized by three properties:\n",
        "* Whether they had down's syndrome (trisomy) or not\n",
        "* Whether they were stimulated to learn or not\n",
        "* Whether they had a drug memantine or a saline control solution.\n",
        "\n",
        "With these three choices, there are 8 possible classes for each mouse.  For each mouse, the expression levels were measured across 77 genes.  We will see if the characteristics can be predicted from the gene expression levels.  This classification could reveal which genes are potentially involved in Down's syndrome and if drugs and learning have any noticeable effects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbBIxhlXbkPI"
      },
      "source": [
        "## Load the Data\n",
        "\n",
        "We begin by loading the standard modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GIj1t3abkPJ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn import linear_model, preprocessing"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJHSXmF0bkPK"
      },
      "source": [
        "Use the `pd.read_excel` command to read the data from \n",
        "\n",
        "https://archive.ics.uci.edu/ml/machine-learning-databases/00342/Data_Cortex_Nuclear.xls\n",
        "\n",
        "into a dataframe `df`.  Use the `index_col` option to specify that column 0 is the index.  Use the `df.head()` to print the first few rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M19F7QLbkPK"
      },
      "source": [
        "# TODO 1\n",
        "df = pd.read_excel('https://archive.ics.uci.edu/ml/machine-learning-databases/00342/Data_Cortex_Nuclear.xls',index_col=0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y78G4mWhbkPL"
      },
      "source": [
        "This data has missing values.  The site:\n",
        "\n",
        "http://pandas.pydata.org/pandas-docs/stable/missing_data.html\n",
        "\n",
        "has an excellent summary of methods to deal with missing values.  Following the techniques there, create a new data frame `df1` where the missing values in each column are filled with the mean values from the non-missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqyV4aB9bkPL"
      },
      "source": [
        "# TODO 2\n",
        "df1=df.fillna(df.mean())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XTEjel8bkPM"
      },
      "source": [
        "## Binary Classification for Down's Syndrome\n",
        "\n",
        "We will first predict the binary class label in `df1['Genotype']` which indicates if the mouse has Down's syndrome or not.  Get the string values in `df1['Genotype'].values` and convert this to a numeric vector `y` with 0 or 1.  You may wish to use the `np.unique` command with the `return_inverse=True` option."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8BhSWZFbkPM",
        "outputId": "401c2458-7f04-4fdc-f85b-8b286343ef80"
      },
      "source": [
        "# TODO 3\n",
        "str_list,y=np.unique(df1['Genotype'].values,return_inverse=True)\n",
        "y"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8PGgEZ4bkPN"
      },
      "source": [
        "As predictors, get all but the last four columns of the dataframes.  Store the data matrix into `X` and the names of the columns in `xnames`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "6WXeoQYbbkPO",
        "outputId": "666e0c0c-b774-4303-bb8d-ba931ca0dffb"
      },
      "source": [
        "# TODO 4\n",
        "xnames=df1.columns[0:-4]\n",
        "X=df1[xnames]\n",
        "X"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DYRK1A_N</th>\n",
              "      <th>ITSN1_N</th>\n",
              "      <th>BDNF_N</th>\n",
              "      <th>NR1_N</th>\n",
              "      <th>NR2A_N</th>\n",
              "      <th>pAKT_N</th>\n",
              "      <th>pBRAF_N</th>\n",
              "      <th>pCAMKII_N</th>\n",
              "      <th>pCREB_N</th>\n",
              "      <th>pELK_N</th>\n",
              "      <th>pERK_N</th>\n",
              "      <th>pJNK_N</th>\n",
              "      <th>PKCA_N</th>\n",
              "      <th>pMEK_N</th>\n",
              "      <th>pNR1_N</th>\n",
              "      <th>pNR2A_N</th>\n",
              "      <th>pNR2B_N</th>\n",
              "      <th>pPKCAB_N</th>\n",
              "      <th>pRSK_N</th>\n",
              "      <th>AKT_N</th>\n",
              "      <th>BRAF_N</th>\n",
              "      <th>CAMKII_N</th>\n",
              "      <th>CREB_N</th>\n",
              "      <th>ELK_N</th>\n",
              "      <th>ERK_N</th>\n",
              "      <th>GSK3B_N</th>\n",
              "      <th>JNK_N</th>\n",
              "      <th>MEK_N</th>\n",
              "      <th>TRKA_N</th>\n",
              "      <th>RSK_N</th>\n",
              "      <th>APP_N</th>\n",
              "      <th>Bcatenin_N</th>\n",
              "      <th>SOD1_N</th>\n",
              "      <th>MTOR_N</th>\n",
              "      <th>P38_N</th>\n",
              "      <th>pMTOR_N</th>\n",
              "      <th>DSCR1_N</th>\n",
              "      <th>AMPKA_N</th>\n",
              "      <th>NR2B_N</th>\n",
              "      <th>pNUMB_N</th>\n",
              "      <th>RAPTOR_N</th>\n",
              "      <th>TIAM1_N</th>\n",
              "      <th>pP70S6_N</th>\n",
              "      <th>NUMB_N</th>\n",
              "      <th>P70S6_N</th>\n",
              "      <th>pGSK3B_N</th>\n",
              "      <th>pPKCG_N</th>\n",
              "      <th>CDK5_N</th>\n",
              "      <th>S6_N</th>\n",
              "      <th>ADARB1_N</th>\n",
              "      <th>AcetylH3K9_N</th>\n",
              "      <th>RRP1_N</th>\n",
              "      <th>BAX_N</th>\n",
              "      <th>ARC_N</th>\n",
              "      <th>ERBB4_N</th>\n",
              "      <th>nNOS_N</th>\n",
              "      <th>Tau_N</th>\n",
              "      <th>GFAP_N</th>\n",
              "      <th>GluR3_N</th>\n",
              "      <th>GluR4_N</th>\n",
              "      <th>IL1B_N</th>\n",
              "      <th>P3525_N</th>\n",
              "      <th>pCASP9_N</th>\n",
              "      <th>PSD95_N</th>\n",
              "      <th>SNCA_N</th>\n",
              "      <th>Ubiquitin_N</th>\n",
              "      <th>pGSK3B_Tyr216_N</th>\n",
              "      <th>SHH_N</th>\n",
              "      <th>BAD_N</th>\n",
              "      <th>BCL2_N</th>\n",
              "      <th>pS6_N</th>\n",
              "      <th>pCFOS_N</th>\n",
              "      <th>SYP_N</th>\n",
              "      <th>H3AcK18_N</th>\n",
              "      <th>EGR1_N</th>\n",
              "      <th>H3MeK4_N</th>\n",
              "      <th>CaNA_N</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MouseID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>309_1</th>\n",
              "      <td>0.503644</td>\n",
              "      <td>0.747193</td>\n",
              "      <td>0.430175</td>\n",
              "      <td>2.816329</td>\n",
              "      <td>5.990152</td>\n",
              "      <td>0.218830</td>\n",
              "      <td>0.177565</td>\n",
              "      <td>2.373744</td>\n",
              "      <td>0.232224</td>\n",
              "      <td>1.750936</td>\n",
              "      <td>0.687906</td>\n",
              "      <td>0.306382</td>\n",
              "      <td>0.402698</td>\n",
              "      <td>0.296927</td>\n",
              "      <td>1.022060</td>\n",
              "      <td>0.605673</td>\n",
              "      <td>1.877684</td>\n",
              "      <td>2.308745</td>\n",
              "      <td>0.441599</td>\n",
              "      <td>0.859366</td>\n",
              "      <td>0.416289</td>\n",
              "      <td>0.369608</td>\n",
              "      <td>0.178944</td>\n",
              "      <td>1.866358</td>\n",
              "      <td>3.685247</td>\n",
              "      <td>1.537227</td>\n",
              "      <td>0.264526</td>\n",
              "      <td>0.319677</td>\n",
              "      <td>0.813866</td>\n",
              "      <td>0.165846</td>\n",
              "      <td>0.453910</td>\n",
              "      <td>3.037621</td>\n",
              "      <td>0.369510</td>\n",
              "      <td>0.458539</td>\n",
              "      <td>0.335336</td>\n",
              "      <td>0.825192</td>\n",
              "      <td>0.576916</td>\n",
              "      <td>0.448099</td>\n",
              "      <td>0.586271</td>\n",
              "      <td>0.394721</td>\n",
              "      <td>0.339571</td>\n",
              "      <td>0.482864</td>\n",
              "      <td>0.294170</td>\n",
              "      <td>0.182150</td>\n",
              "      <td>0.842725</td>\n",
              "      <td>0.192608</td>\n",
              "      <td>1.443091</td>\n",
              "      <td>0.294700</td>\n",
              "      <td>0.354605</td>\n",
              "      <td>1.339070</td>\n",
              "      <td>0.170119</td>\n",
              "      <td>0.159102</td>\n",
              "      <td>0.188852</td>\n",
              "      <td>0.106305</td>\n",
              "      <td>0.144989</td>\n",
              "      <td>0.176668</td>\n",
              "      <td>0.125190</td>\n",
              "      <td>0.115291</td>\n",
              "      <td>0.228043</td>\n",
              "      <td>0.142756</td>\n",
              "      <td>0.430957</td>\n",
              "      <td>0.247538</td>\n",
              "      <td>1.603310</td>\n",
              "      <td>2.014875</td>\n",
              "      <td>0.108234</td>\n",
              "      <td>1.044979</td>\n",
              "      <td>0.831557</td>\n",
              "      <td>0.188852</td>\n",
              "      <td>0.122652</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.106305</td>\n",
              "      <td>0.108336</td>\n",
              "      <td>0.427099</td>\n",
              "      <td>0.114783</td>\n",
              "      <td>0.131790</td>\n",
              "      <td>0.128186</td>\n",
              "      <td>1.675652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309_2</th>\n",
              "      <td>0.514617</td>\n",
              "      <td>0.689064</td>\n",
              "      <td>0.411770</td>\n",
              "      <td>2.789514</td>\n",
              "      <td>5.685038</td>\n",
              "      <td>0.211636</td>\n",
              "      <td>0.172817</td>\n",
              "      <td>2.292150</td>\n",
              "      <td>0.226972</td>\n",
              "      <td>1.596377</td>\n",
              "      <td>0.695006</td>\n",
              "      <td>0.299051</td>\n",
              "      <td>0.385987</td>\n",
              "      <td>0.281319</td>\n",
              "      <td>0.956676</td>\n",
              "      <td>0.587559</td>\n",
              "      <td>1.725774</td>\n",
              "      <td>2.043037</td>\n",
              "      <td>0.445222</td>\n",
              "      <td>0.834659</td>\n",
              "      <td>0.400364</td>\n",
              "      <td>0.356178</td>\n",
              "      <td>0.173680</td>\n",
              "      <td>1.761047</td>\n",
              "      <td>3.485287</td>\n",
              "      <td>1.509249</td>\n",
              "      <td>0.255727</td>\n",
              "      <td>0.304419</td>\n",
              "      <td>0.780504</td>\n",
              "      <td>0.157194</td>\n",
              "      <td>0.430940</td>\n",
              "      <td>2.921882</td>\n",
              "      <td>0.342279</td>\n",
              "      <td>0.423560</td>\n",
              "      <td>0.324835</td>\n",
              "      <td>0.761718</td>\n",
              "      <td>0.545097</td>\n",
              "      <td>0.420876</td>\n",
              "      <td>0.545097</td>\n",
              "      <td>0.368255</td>\n",
              "      <td>0.321959</td>\n",
              "      <td>0.454519</td>\n",
              "      <td>0.276431</td>\n",
              "      <td>0.182086</td>\n",
              "      <td>0.847615</td>\n",
              "      <td>0.194815</td>\n",
              "      <td>1.439460</td>\n",
              "      <td>0.294060</td>\n",
              "      <td>0.354548</td>\n",
              "      <td>1.306323</td>\n",
              "      <td>0.171427</td>\n",
              "      <td>0.158129</td>\n",
              "      <td>0.184570</td>\n",
              "      <td>0.106592</td>\n",
              "      <td>0.150471</td>\n",
              "      <td>0.178309</td>\n",
              "      <td>0.134275</td>\n",
              "      <td>0.118235</td>\n",
              "      <td>0.238073</td>\n",
              "      <td>0.142037</td>\n",
              "      <td>0.457156</td>\n",
              "      <td>0.257632</td>\n",
              "      <td>1.671738</td>\n",
              "      <td>2.004605</td>\n",
              "      <td>0.109749</td>\n",
              "      <td>1.009883</td>\n",
              "      <td>0.849270</td>\n",
              "      <td>0.200404</td>\n",
              "      <td>0.116682</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.106592</td>\n",
              "      <td>0.104315</td>\n",
              "      <td>0.441581</td>\n",
              "      <td>0.111974</td>\n",
              "      <td>0.135103</td>\n",
              "      <td>0.131119</td>\n",
              "      <td>1.743610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309_3</th>\n",
              "      <td>0.509183</td>\n",
              "      <td>0.730247</td>\n",
              "      <td>0.418309</td>\n",
              "      <td>2.687201</td>\n",
              "      <td>5.622059</td>\n",
              "      <td>0.209011</td>\n",
              "      <td>0.175722</td>\n",
              "      <td>2.283337</td>\n",
              "      <td>0.230247</td>\n",
              "      <td>1.561316</td>\n",
              "      <td>0.677348</td>\n",
              "      <td>0.291276</td>\n",
              "      <td>0.381002</td>\n",
              "      <td>0.281710</td>\n",
              "      <td>1.003635</td>\n",
              "      <td>0.602449</td>\n",
              "      <td>1.731873</td>\n",
              "      <td>2.017984</td>\n",
              "      <td>0.467668</td>\n",
              "      <td>0.814329</td>\n",
              "      <td>0.399847</td>\n",
              "      <td>0.368089</td>\n",
              "      <td>0.173905</td>\n",
              "      <td>1.765544</td>\n",
              "      <td>3.571456</td>\n",
              "      <td>1.501244</td>\n",
              "      <td>0.259614</td>\n",
              "      <td>0.311747</td>\n",
              "      <td>0.785154</td>\n",
              "      <td>0.160895</td>\n",
              "      <td>0.423187</td>\n",
              "      <td>2.944136</td>\n",
              "      <td>0.343696</td>\n",
              "      <td>0.425005</td>\n",
              "      <td>0.324852</td>\n",
              "      <td>0.757031</td>\n",
              "      <td>0.543620</td>\n",
              "      <td>0.404630</td>\n",
              "      <td>0.552994</td>\n",
              "      <td>0.363880</td>\n",
              "      <td>0.313086</td>\n",
              "      <td>0.447197</td>\n",
              "      <td>0.256648</td>\n",
              "      <td>0.184388</td>\n",
              "      <td>0.856166</td>\n",
              "      <td>0.200737</td>\n",
              "      <td>1.524364</td>\n",
              "      <td>0.301881</td>\n",
              "      <td>0.386087</td>\n",
              "      <td>1.279600</td>\n",
              "      <td>0.185456</td>\n",
              "      <td>0.148696</td>\n",
              "      <td>0.190532</td>\n",
              "      <td>0.108303</td>\n",
              "      <td>0.145330</td>\n",
              "      <td>0.176213</td>\n",
              "      <td>0.132560</td>\n",
              "      <td>0.117760</td>\n",
              "      <td>0.244817</td>\n",
              "      <td>0.142445</td>\n",
              "      <td>0.510472</td>\n",
              "      <td>0.255343</td>\n",
              "      <td>1.663550</td>\n",
              "      <td>2.016831</td>\n",
              "      <td>0.108196</td>\n",
              "      <td>0.996848</td>\n",
              "      <td>0.846709</td>\n",
              "      <td>0.193685</td>\n",
              "      <td>0.118508</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.108303</td>\n",
              "      <td>0.106219</td>\n",
              "      <td>0.435777</td>\n",
              "      <td>0.111883</td>\n",
              "      <td>0.133362</td>\n",
              "      <td>0.127431</td>\n",
              "      <td>1.926427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309_4</th>\n",
              "      <td>0.442107</td>\n",
              "      <td>0.617076</td>\n",
              "      <td>0.358626</td>\n",
              "      <td>2.466947</td>\n",
              "      <td>4.979503</td>\n",
              "      <td>0.222886</td>\n",
              "      <td>0.176463</td>\n",
              "      <td>2.152301</td>\n",
              "      <td>0.207004</td>\n",
              "      <td>1.595086</td>\n",
              "      <td>0.583277</td>\n",
              "      <td>0.296729</td>\n",
              "      <td>0.377087</td>\n",
              "      <td>0.313832</td>\n",
              "      <td>0.875390</td>\n",
              "      <td>0.520293</td>\n",
              "      <td>1.566852</td>\n",
              "      <td>2.132754</td>\n",
              "      <td>0.477671</td>\n",
              "      <td>0.727705</td>\n",
              "      <td>0.385639</td>\n",
              "      <td>0.362970</td>\n",
              "      <td>0.179449</td>\n",
              "      <td>1.286277</td>\n",
              "      <td>2.970137</td>\n",
              "      <td>1.419710</td>\n",
              "      <td>0.259536</td>\n",
              "      <td>0.279218</td>\n",
              "      <td>0.734492</td>\n",
              "      <td>0.162210</td>\n",
              "      <td>0.410615</td>\n",
              "      <td>2.500204</td>\n",
              "      <td>0.344509</td>\n",
              "      <td>0.429211</td>\n",
              "      <td>0.330121</td>\n",
              "      <td>0.746980</td>\n",
              "      <td>0.546763</td>\n",
              "      <td>0.386860</td>\n",
              "      <td>0.547849</td>\n",
              "      <td>0.366771</td>\n",
              "      <td>0.328492</td>\n",
              "      <td>0.442650</td>\n",
              "      <td>0.398534</td>\n",
              "      <td>0.161768</td>\n",
              "      <td>0.760234</td>\n",
              "      <td>0.184169</td>\n",
              "      <td>1.612382</td>\n",
              "      <td>0.296382</td>\n",
              "      <td>0.290680</td>\n",
              "      <td>1.198765</td>\n",
              "      <td>0.159799</td>\n",
              "      <td>0.166112</td>\n",
              "      <td>0.185323</td>\n",
              "      <td>0.103184</td>\n",
              "      <td>0.140656</td>\n",
              "      <td>0.163804</td>\n",
              "      <td>0.123210</td>\n",
              "      <td>0.117439</td>\n",
              "      <td>0.234947</td>\n",
              "      <td>0.145068</td>\n",
              "      <td>0.430996</td>\n",
              "      <td>0.251103</td>\n",
              "      <td>1.484624</td>\n",
              "      <td>1.957233</td>\n",
              "      <td>0.119883</td>\n",
              "      <td>0.990225</td>\n",
              "      <td>0.833277</td>\n",
              "      <td>0.192112</td>\n",
              "      <td>0.132781</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.103184</td>\n",
              "      <td>0.111262</td>\n",
              "      <td>0.391691</td>\n",
              "      <td>0.130405</td>\n",
              "      <td>0.147444</td>\n",
              "      <td>0.146901</td>\n",
              "      <td>1.700563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309_5</th>\n",
              "      <td>0.434940</td>\n",
              "      <td>0.617430</td>\n",
              "      <td>0.358802</td>\n",
              "      <td>2.365785</td>\n",
              "      <td>4.718679</td>\n",
              "      <td>0.213106</td>\n",
              "      <td>0.173627</td>\n",
              "      <td>2.134014</td>\n",
              "      <td>0.192158</td>\n",
              "      <td>1.504230</td>\n",
              "      <td>0.550960</td>\n",
              "      <td>0.286961</td>\n",
              "      <td>0.363502</td>\n",
              "      <td>0.277964</td>\n",
              "      <td>0.864912</td>\n",
              "      <td>0.507990</td>\n",
              "      <td>1.480059</td>\n",
              "      <td>2.013697</td>\n",
              "      <td>0.483416</td>\n",
              "      <td>0.687794</td>\n",
              "      <td>0.367531</td>\n",
              "      <td>0.355311</td>\n",
              "      <td>0.174836</td>\n",
              "      <td>1.324695</td>\n",
              "      <td>2.896334</td>\n",
              "      <td>1.359876</td>\n",
              "      <td>0.250705</td>\n",
              "      <td>0.273667</td>\n",
              "      <td>0.702699</td>\n",
              "      <td>0.154827</td>\n",
              "      <td>0.398550</td>\n",
              "      <td>2.456560</td>\n",
              "      <td>0.329126</td>\n",
              "      <td>0.408755</td>\n",
              "      <td>0.313415</td>\n",
              "      <td>0.691956</td>\n",
              "      <td>0.536860</td>\n",
              "      <td>0.360816</td>\n",
              "      <td>0.512824</td>\n",
              "      <td>0.351551</td>\n",
              "      <td>0.312206</td>\n",
              "      <td>0.419095</td>\n",
              "      <td>0.393447</td>\n",
              "      <td>0.160200</td>\n",
              "      <td>0.768113</td>\n",
              "      <td>0.185718</td>\n",
              "      <td>1.645807</td>\n",
              "      <td>0.296829</td>\n",
              "      <td>0.309345</td>\n",
              "      <td>1.206995</td>\n",
              "      <td>0.164650</td>\n",
              "      <td>0.160687</td>\n",
              "      <td>0.188221</td>\n",
              "      <td>0.104784</td>\n",
              "      <td>0.141983</td>\n",
              "      <td>0.167710</td>\n",
              "      <td>0.136838</td>\n",
              "      <td>0.116048</td>\n",
              "      <td>0.255528</td>\n",
              "      <td>0.140871</td>\n",
              "      <td>0.481227</td>\n",
              "      <td>0.251773</td>\n",
              "      <td>1.534835</td>\n",
              "      <td>2.009109</td>\n",
              "      <td>0.119524</td>\n",
              "      <td>0.997775</td>\n",
              "      <td>0.878668</td>\n",
              "      <td>0.205604</td>\n",
              "      <td>0.129954</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.104784</td>\n",
              "      <td>0.110694</td>\n",
              "      <td>0.434154</td>\n",
              "      <td>0.118481</td>\n",
              "      <td>0.140314</td>\n",
              "      <td>0.148380</td>\n",
              "      <td>1.839730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>J3295_11</th>\n",
              "      <td>0.254860</td>\n",
              "      <td>0.463591</td>\n",
              "      <td>0.254860</td>\n",
              "      <td>2.092082</td>\n",
              "      <td>2.600035</td>\n",
              "      <td>0.211736</td>\n",
              "      <td>0.171262</td>\n",
              "      <td>2.483740</td>\n",
              "      <td>0.207317</td>\n",
              "      <td>1.057971</td>\n",
              "      <td>0.265642</td>\n",
              "      <td>0.294097</td>\n",
              "      <td>0.249912</td>\n",
              "      <td>0.261223</td>\n",
              "      <td>0.746200</td>\n",
              "      <td>0.510604</td>\n",
              "      <td>1.220926</td>\n",
              "      <td>1.241958</td>\n",
              "      <td>0.422764</td>\n",
              "      <td>0.638211</td>\n",
              "      <td>0.255744</td>\n",
              "      <td>0.330859</td>\n",
              "      <td>0.190173</td>\n",
              "      <td>0.896430</td>\n",
              "      <td>1.822906</td>\n",
              "      <td>0.993107</td>\n",
              "      <td>0.236303</td>\n",
              "      <td>0.249205</td>\n",
              "      <td>0.641746</td>\n",
              "      <td>0.165606</td>\n",
              "      <td>0.372216</td>\n",
              "      <td>1.828208</td>\n",
              "      <td>0.820078</td>\n",
              "      <td>0.380170</td>\n",
              "      <td>0.320431</td>\n",
              "      <td>0.599328</td>\n",
              "      <td>0.508130</td>\n",
              "      <td>0.270060</td>\n",
              "      <td>0.437964</td>\n",
              "      <td>0.260516</td>\n",
              "      <td>0.239130</td>\n",
              "      <td>0.340756</td>\n",
              "      <td>0.527041</td>\n",
              "      <td>0.209433</td>\n",
              "      <td>0.787900</td>\n",
              "      <td>0.178130</td>\n",
              "      <td>2.630825</td>\n",
              "      <td>0.319062</td>\n",
              "      <td>0.654548</td>\n",
              "      <td>0.737226</td>\n",
              "      <td>0.532987</td>\n",
              "      <td>0.196659</td>\n",
              "      <td>0.182762</td>\n",
              "      <td>0.115806</td>\n",
              "      <td>0.160303</td>\n",
              "      <td>0.189360</td>\n",
              "      <td>0.411286</td>\n",
              "      <td>0.134896</td>\n",
              "      <td>0.207748</td>\n",
              "      <td>0.134475</td>\n",
              "      <td>0.503650</td>\n",
              "      <td>0.326362</td>\n",
              "      <td>1.323554</td>\n",
              "      <td>2.578046</td>\n",
              "      <td>0.167181</td>\n",
              "      <td>1.261651</td>\n",
              "      <td>0.962942</td>\n",
              "      <td>0.275547</td>\n",
              "      <td>0.190483</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.115806</td>\n",
              "      <td>0.183324</td>\n",
              "      <td>0.374088</td>\n",
              "      <td>0.318782</td>\n",
              "      <td>0.204660</td>\n",
              "      <td>0.328327</td>\n",
              "      <td>1.364823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>J3295_12</th>\n",
              "      <td>0.272198</td>\n",
              "      <td>0.474163</td>\n",
              "      <td>0.251638</td>\n",
              "      <td>2.161390</td>\n",
              "      <td>2.801492</td>\n",
              "      <td>0.251274</td>\n",
              "      <td>0.182496</td>\n",
              "      <td>2.512737</td>\n",
              "      <td>0.216339</td>\n",
              "      <td>1.081150</td>\n",
              "      <td>0.270378</td>\n",
              "      <td>0.285116</td>\n",
              "      <td>0.249818</td>\n",
              "      <td>0.252547</td>\n",
              "      <td>0.749818</td>\n",
              "      <td>0.524381</td>\n",
              "      <td>1.218705</td>\n",
              "      <td>1.361354</td>\n",
              "      <td>0.415211</td>\n",
              "      <td>0.645197</td>\n",
              "      <td>0.252001</td>\n",
              "      <td>0.338610</td>\n",
              "      <td>0.181223</td>\n",
              "      <td>0.958879</td>\n",
              "      <td>1.879913</td>\n",
              "      <td>0.974891</td>\n",
              "      <td>0.245451</td>\n",
              "      <td>0.262191</td>\n",
              "      <td>0.693595</td>\n",
              "      <td>0.191594</td>\n",
              "      <td>0.360990</td>\n",
              "      <td>1.883370</td>\n",
              "      <td>0.854258</td>\n",
              "      <td>0.380277</td>\n",
              "      <td>0.338246</td>\n",
              "      <td>0.614629</td>\n",
              "      <td>0.519105</td>\n",
              "      <td>0.273472</td>\n",
              "      <td>0.580058</td>\n",
              "      <td>0.275837</td>\n",
              "      <td>0.235080</td>\n",
              "      <td>0.346252</td>\n",
              "      <td>0.518377</td>\n",
              "      <td>0.194333</td>\n",
              "      <td>0.763096</td>\n",
              "      <td>0.170422</td>\n",
              "      <td>2.593227</td>\n",
              "      <td>0.318867</td>\n",
              "      <td>0.632066</td>\n",
              "      <td>0.756047</td>\n",
              "      <td>0.546648</td>\n",
              "      <td>0.188390</td>\n",
              "      <td>0.166966</td>\n",
              "      <td>0.113614</td>\n",
              "      <td>0.161576</td>\n",
              "      <td>0.187146</td>\n",
              "      <td>0.402073</td>\n",
              "      <td>0.130615</td>\n",
              "      <td>0.205114</td>\n",
              "      <td>0.122184</td>\n",
              "      <td>0.512647</td>\n",
              "      <td>0.344160</td>\n",
              "      <td>1.275605</td>\n",
              "      <td>2.534347</td>\n",
              "      <td>0.169592</td>\n",
              "      <td>1.254872</td>\n",
              "      <td>0.983690</td>\n",
              "      <td>0.283207</td>\n",
              "      <td>0.190463</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.113614</td>\n",
              "      <td>0.175674</td>\n",
              "      <td>0.375259</td>\n",
              "      <td>0.325639</td>\n",
              "      <td>0.200415</td>\n",
              "      <td>0.293435</td>\n",
              "      <td>1.364478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>J3295_13</th>\n",
              "      <td>0.228700</td>\n",
              "      <td>0.395179</td>\n",
              "      <td>0.234118</td>\n",
              "      <td>1.733184</td>\n",
              "      <td>2.220852</td>\n",
              "      <td>0.220665</td>\n",
              "      <td>0.161435</td>\n",
              "      <td>1.989723</td>\n",
              "      <td>0.185164</td>\n",
              "      <td>0.884342</td>\n",
              "      <td>0.255045</td>\n",
              "      <td>0.245703</td>\n",
              "      <td>0.221413</td>\n",
              "      <td>0.255792</td>\n",
              "      <td>0.636024</td>\n",
              "      <td>0.442638</td>\n",
              "      <td>1.015695</td>\n",
              "      <td>1.065022</td>\n",
              "      <td>0.408259</td>\n",
              "      <td>0.540172</td>\n",
              "      <td>0.238042</td>\n",
              "      <td>0.326981</td>\n",
              "      <td>0.212631</td>\n",
              "      <td>0.762892</td>\n",
              "      <td>1.425262</td>\n",
              "      <td>0.818199</td>\n",
              "      <td>0.216741</td>\n",
              "      <td>0.235426</td>\n",
              "      <td>0.559043</td>\n",
              "      <td>0.168161</td>\n",
              "      <td>0.309978</td>\n",
              "      <td>1.494208</td>\n",
              "      <td>0.661809</td>\n",
              "      <td>0.337444</td>\n",
              "      <td>0.309978</td>\n",
              "      <td>0.510650</td>\n",
              "      <td>0.463378</td>\n",
              "      <td>0.234679</td>\n",
              "      <td>0.400037</td>\n",
              "      <td>0.235239</td>\n",
              "      <td>0.231876</td>\n",
              "      <td>0.292788</td>\n",
              "      <td>0.460202</td>\n",
              "      <td>0.196736</td>\n",
              "      <td>0.804896</td>\n",
              "      <td>0.170807</td>\n",
              "      <td>2.628286</td>\n",
              "      <td>0.313327</td>\n",
              "      <td>0.669810</td>\n",
              "      <td>0.764098</td>\n",
              "      <td>0.536899</td>\n",
              "      <td>0.201269</td>\n",
              "      <td>0.169175</td>\n",
              "      <td>0.118948</td>\n",
              "      <td>0.174252</td>\n",
              "      <td>0.185131</td>\n",
              "      <td>0.395648</td>\n",
              "      <td>0.137081</td>\n",
              "      <td>0.201088</td>\n",
              "      <td>0.126927</td>\n",
              "      <td>0.631188</td>\n",
              "      <td>0.358114</td>\n",
              "      <td>1.437534</td>\n",
              "      <td>2.544515</td>\n",
              "      <td>0.179692</td>\n",
              "      <td>1.242248</td>\n",
              "      <td>0.976609</td>\n",
              "      <td>0.290843</td>\n",
              "      <td>0.216682</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.118948</td>\n",
              "      <td>0.158296</td>\n",
              "      <td>0.422121</td>\n",
              "      <td>0.321306</td>\n",
              "      <td>0.229193</td>\n",
              "      <td>0.355213</td>\n",
              "      <td>1.430825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>J3295_14</th>\n",
              "      <td>0.221242</td>\n",
              "      <td>0.412894</td>\n",
              "      <td>0.243974</td>\n",
              "      <td>1.876347</td>\n",
              "      <td>2.384088</td>\n",
              "      <td>0.208897</td>\n",
              "      <td>0.173623</td>\n",
              "      <td>2.086028</td>\n",
              "      <td>0.192044</td>\n",
              "      <td>0.922595</td>\n",
              "      <td>0.230649</td>\n",
              "      <td>0.263179</td>\n",
              "      <td>0.224378</td>\n",
              "      <td>0.277484</td>\n",
              "      <td>0.665099</td>\n",
              "      <td>0.479522</td>\n",
              "      <td>1.077014</td>\n",
              "      <td>1.115030</td>\n",
              "      <td>0.434646</td>\n",
              "      <td>0.564766</td>\n",
              "      <td>0.256712</td>\n",
              "      <td>0.313149</td>\n",
              "      <td>0.172252</td>\n",
              "      <td>0.781893</td>\n",
              "      <td>1.558887</td>\n",
              "      <td>0.872820</td>\n",
              "      <td>0.238095</td>\n",
              "      <td>0.270429</td>\n",
              "      <td>0.585146</td>\n",
              "      <td>0.163433</td>\n",
              "      <td>0.341172</td>\n",
              "      <td>1.571820</td>\n",
              "      <td>0.698413</td>\n",
              "      <td>0.356457</td>\n",
              "      <td>0.308446</td>\n",
              "      <td>0.545757</td>\n",
              "      <td>0.480502</td>\n",
              "      <td>0.258083</td>\n",
              "      <td>0.426612</td>\n",
              "      <td>0.258475</td>\n",
              "      <td>0.215951</td>\n",
              "      <td>0.301587</td>\n",
              "      <td>0.490496</td>\n",
              "      <td>0.197567</td>\n",
              "      <td>0.784819</td>\n",
              "      <td>0.175413</td>\n",
              "      <td>2.659706</td>\n",
              "      <td>0.341021</td>\n",
              "      <td>0.642637</td>\n",
              "      <td>0.783185</td>\n",
              "      <td>0.538224</td>\n",
              "      <td>0.212094</td>\n",
              "      <td>0.161431</td>\n",
              "      <td>0.125295</td>\n",
              "      <td>0.172508</td>\n",
              "      <td>0.193753</td>\n",
              "      <td>0.414200</td>\n",
              "      <td>0.149265</td>\n",
              "      <td>0.204467</td>\n",
              "      <td>0.124569</td>\n",
              "      <td>0.621754</td>\n",
              "      <td>0.352279</td>\n",
              "      <td>1.498820</td>\n",
              "      <td>2.609769</td>\n",
              "      <td>0.185037</td>\n",
              "      <td>1.301071</td>\n",
              "      <td>0.989286</td>\n",
              "      <td>0.306701</td>\n",
              "      <td>0.222263</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.125295</td>\n",
              "      <td>0.196296</td>\n",
              "      <td>0.397676</td>\n",
              "      <td>0.335936</td>\n",
              "      <td>0.251317</td>\n",
              "      <td>0.365353</td>\n",
              "      <td>1.404031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>J3295_15</th>\n",
              "      <td>0.302626</td>\n",
              "      <td>0.461059</td>\n",
              "      <td>0.256564</td>\n",
              "      <td>2.092790</td>\n",
              "      <td>2.594348</td>\n",
              "      <td>0.251001</td>\n",
              "      <td>0.191811</td>\n",
              "      <td>2.361816</td>\n",
              "      <td>0.223632</td>\n",
              "      <td>1.064085</td>\n",
              "      <td>0.276146</td>\n",
              "      <td>0.293725</td>\n",
              "      <td>0.275478</td>\n",
              "      <td>0.315754</td>\n",
              "      <td>0.713173</td>\n",
              "      <td>0.502225</td>\n",
              "      <td>1.231865</td>\n",
              "      <td>1.282377</td>\n",
              "      <td>0.429239</td>\n",
              "      <td>0.629506</td>\n",
              "      <td>0.311972</td>\n",
              "      <td>0.349800</td>\n",
              "      <td>0.187583</td>\n",
              "      <td>0.884735</td>\n",
              "      <td>1.785937</td>\n",
              "      <td>0.970182</td>\n",
              "      <td>0.287049</td>\n",
              "      <td>0.281709</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.190031</td>\n",
              "      <td>0.402982</td>\n",
              "      <td>1.742768</td>\n",
              "      <td>0.786827</td>\n",
              "      <td>0.399866</td>\n",
              "      <td>0.339119</td>\n",
              "      <td>0.602804</td>\n",
              "      <td>0.533823</td>\n",
              "      <td>0.287717</td>\n",
              "      <td>0.637962</td>\n",
              "      <td>0.281486</td>\n",
              "      <td>0.273698</td>\n",
              "      <td>0.368269</td>\n",
              "      <td>0.546729</td>\n",
              "      <td>0.188807</td>\n",
              "      <td>0.772752</td>\n",
              "      <td>0.172716</td>\n",
              "      <td>2.654926</td>\n",
              "      <td>0.317003</td>\n",
              "      <td>0.631682</td>\n",
              "      <td>0.736635</td>\n",
              "      <td>0.535312</td>\n",
              "      <td>0.193992</td>\n",
              "      <td>0.172180</td>\n",
              "      <td>0.118899</td>\n",
              "      <td>0.172001</td>\n",
              "      <td>0.191311</td>\n",
              "      <td>0.393170</td>\n",
              "      <td>0.140533</td>\n",
              "      <td>0.204363</td>\n",
              "      <td>0.121402</td>\n",
              "      <td>0.630252</td>\n",
              "      <td>0.356517</td>\n",
              "      <td>1.490077</td>\n",
              "      <td>2.526372</td>\n",
              "      <td>0.184516</td>\n",
              "      <td>1.267120</td>\n",
              "      <td>1.020383</td>\n",
              "      <td>0.292330</td>\n",
              "      <td>0.227606</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.118899</td>\n",
              "      <td>0.187556</td>\n",
              "      <td>0.420347</td>\n",
              "      <td>0.335062</td>\n",
              "      <td>0.252995</td>\n",
              "      <td>0.365278</td>\n",
              "      <td>1.370999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1080 rows × 77 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          DYRK1A_N   ITSN1_N    BDNF_N  ...    EGR1_N  H3MeK4_N    CaNA_N\n",
              "MouseID                                 ...                              \n",
              "309_1     0.503644  0.747193  0.430175  ...  0.131790  0.128186  1.675652\n",
              "309_2     0.514617  0.689064  0.411770  ...  0.135103  0.131119  1.743610\n",
              "309_3     0.509183  0.730247  0.418309  ...  0.133362  0.127431  1.926427\n",
              "309_4     0.442107  0.617076  0.358626  ...  0.147444  0.146901  1.700563\n",
              "309_5     0.434940  0.617430  0.358802  ...  0.140314  0.148380  1.839730\n",
              "...            ...       ...       ...  ...       ...       ...       ...\n",
              "J3295_11  0.254860  0.463591  0.254860  ...  0.204660  0.328327  1.364823\n",
              "J3295_12  0.272198  0.474163  0.251638  ...  0.200415  0.293435  1.364478\n",
              "J3295_13  0.228700  0.395179  0.234118  ...  0.229193  0.355213  1.430825\n",
              "J3295_14  0.221242  0.412894  0.243974  ...  0.251317  0.365353  1.404031\n",
              "J3295_15  0.302626  0.461059  0.256564  ...  0.252995  0.365278  1.370999\n",
              "\n",
              "[1080 rows x 77 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHc01GDUbkPO"
      },
      "source": [
        "Split the data into training and test with 30% allocated for test.  You can use the train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIC0lhBQbkPO"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Use : shuffle=True, random_state=3 so we all can have same split.\n",
        "# TODO 5: \n",
        "Xtr, Xts, ytr, yts = train_test_split(X,y,shuffle=True,random_state=3)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqZ0s4OCbkPP"
      },
      "source": [
        "Scale the data with the `StandardScaler`.  Store the scaled values in `Xtr1` and `Xts1`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgTY8XrkbkPP"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# TODO 6\n",
        "scal = StandardScaler()\n",
        "Xtr1 = scal.fit_transform(Xtr)\n",
        "Xts1 = scal.transform(Xts)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLE0p0OfbkPP"
      },
      "source": [
        "Create a `LogisticRegression` object `logreg` and `fit` on the scaled training data.  Set the regularization level to `C=1e5` and use the optimizer `solver=liblinear`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VyorQEYbkPQ",
        "outputId": "848901c3-2eaa-4f2f-f25e-ff4b61cf112f"
      },
      "source": [
        "# TODO 7\n",
        "logreg = linear_model.LogisticRegression(C=1e5,solver='liblinear')\n",
        "logreg.fit(Xtr1, ytr)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfqvr1RDbkPQ"
      },
      "source": [
        "Measure the accuracy of the classifer on test data.  You should get around 94%.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYY9tqYIbkPQ",
        "outputId": "cbc97fbc-8b0c-4947-c255-9333468d01f4"
      },
      "source": [
        "# TODO 8\n",
        "yhat = logreg.predict(Xts1)\n",
        "acc = np.mean(yhat == yts)\n",
        "print(\"Accuracy on test data = %f\" % acc)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data = 0.951852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qV27Q8AbkPR"
      },
      "source": [
        "## Interpreting the weight vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ_cHdwhbkPR"
      },
      "source": [
        "Create a stem plot of the coefficients, `W` in the logistic regression model.  Jse the `plt.stem()` function with the `use_line_collection=True` option.  You can get the coefficients from `logreg.coef_`, but you will need to reshape this to a 1D array.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "mBkK8zUPbkPR",
        "outputId": "104b8977-912d-48d1-90bf-bb88001f3974"
      },
      "source": [
        "# TODO 9\n",
        "W = logreg.coef_\n",
        "W_re = W.flatten()\n",
        "plt.stem(W_re,use_line_collection=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<StemContainer object of 3 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFE8VV4MbkPR"
      },
      "source": [
        "You should see that `W[i]` is very large for a few components `i`.  These are the genes that are likely to be most involved in Down's Syndrome.   Below we will use L1 regression to enforce sparsity.  Find the names of the genes for two components `i` where the magnitude of `W[i]` is largest.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3maNoBSsbkPS",
        "outputId": "90a01a8f-ea24-4a94-f9c8-c27535f04d79"
      },
      "source": [
        "# TODO 10\n",
        "idx=np.argmax(W)\n",
        "xnames[idx]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ITSN1_N'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1YFwchIbkPT"
      },
      "source": [
        "## Cross Validation\n",
        "\n",
        "To obtain a slightly more accurate result, now perform 10-fold cross validation and measure the average precision, recall and f1-score.  Note, that in performing the cross-validation, you will want to randomly permute the test and training sets using the `shuffle` option.  In this data set, all the samples from each class are bunched together, so shuffling is essential.  Print the mean precision, recall and f1-score and error rate across all the folds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWKtLqkJbkPT",
        "outputId": "9393b823-ad9e-4932-a8ee-ff6b3b79b506"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "nfold = 10\n",
        "kf = KFold(n_splits=nfold,shuffle=True,random_state=3)\n",
        "# Initial metrics array\n",
        "acc = np.zeros(nfold)\n",
        "prec = np.zeros(nfold)\n",
        "rec = np.zeros(nfold)\n",
        "f1 = np.zeros(nfold)\n",
        "\n",
        "# TODO 11\n",
        "for i, I in enumerate(kf.split(X)):\n",
        "    \n",
        "    # Get training and test data\n",
        "    train, test = I\n",
        "\n",
        "    Xtr = X.values[train,:]\n",
        "    ytr = y[train]\n",
        "    Xts = X.values[test,:]\n",
        "    yts = y[test]\n",
        "    \n",
        "    # Scale the data\n",
        "    scal = StandardScaler()\n",
        "    Xtr1 = scal.fit_transform(Xtr)\n",
        "    Xts1 = scal.transform(Xts)    \n",
        "    \n",
        "    # Fit a model\n",
        "    reg=linear_model.LogisticRegression(C=1e5,solver='liblinear')    \n",
        "    reg.fit(Xtr1, ytr)\n",
        "    \n",
        "    # Predict on test samples and measure accuracy\n",
        "    yhat = reg.predict(Xts1)\n",
        "    acc[i] = np.mean(yhat == yts)\n",
        "    \n",
        "    # Measure other performance metrics\n",
        "    prec[i],rec[i],f1[i],_ = precision_recall_fscore_support(yts,yhat,average='binary')\n",
        "\n",
        "# Take average values of the metrics\n",
        "precm = np.mean(prec)\n",
        "recm = np.mean(rec)\n",
        "f1m = np.mean(f1)\n",
        "accm= np.mean(acc)\n",
        "\n",
        "# Compute the standard errors\n",
        "prec_se = np.std(prec)/np.sqrt(nfold-1)\n",
        "rec_se = np.std(rec)/np.sqrt(nfold-1)\n",
        "f1_se = np.std(f1)/np.sqrt(nfold-1)\n",
        "acc_se = np.std(acc)/np.sqrt(nfold-1)\n",
        "\n",
        "print('Precision = {0:.4f}, SE={1:.4f}'.format(precm,prec_se))\n",
        "print('Recall =    {0:.4f}, SE={1:.4f}'.format(recm, rec_se))\n",
        "print('f1 =        {0:.4f}, SE={1:.4f}'.format(f1m, f1_se))\n",
        "print('Accuracy =  {0:.4f}, SE={1:.4f}'.format(accm, acc_se))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision = 0.9498, SE=0.0118\n",
            "Recall =    0.9593, SE=0.0094\n",
            "f1 =        0.9539, SE=0.0069\n",
            "Accuracy =  0.9565, SE=0.0062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCu9sM0fbkPT"
      },
      "source": [
        "## Multi-Class Classification\n",
        "\n",
        "Now use the response variable in `df1['class']`.  This has 8 possible classes.  Use the `np.unique` funtion as before to convert this to a vector `y` with values 0 to 7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxxfddRNbkPT",
        "outputId": "096b48fe-638b-4182-8082-51382f875c54"
      },
      "source": [
        "# TODO 12\n",
        "str_list,y=np.unique(df1['class'].values,return_inverse=True)\n",
        "str_list,y"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['c-CS-m', 'c-CS-s', 'c-SC-m', 'c-SC-s', 't-CS-m', 't-CS-s',\n",
              "        't-SC-m', 't-SC-s'], dtype=object), array([0, 0, 0, ..., 7, 7, 7]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBR1AQZLbkPT"
      },
      "source": [
        "Fit a multi-class logistic model by creating a `LogisticRegression` object, `logreg` and then calling the `logreg.fit` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP31XfujbkPT"
      },
      "source": [
        "Now perform 10-fold cross validation, and measure the confusion matrix `C` on the test data in each fold. You can use the `confustion_matrix` method in the `sklearn` package.  Add the confusion matrix counts across all folds and then normalize the rows of the confusion matrix so that they sum to one.  Thus, each element `C[i,j]` will represent the fraction of samples where `yhat==j` given `ytrue==i`.  Print the confusion matrix.  You can use the command\n",
        "\n",
        "    print(np.array_str(C, precision=4, suppress_small=True))\n",
        "    \n",
        "to create a nicely formatted print.  Also print the overall mean and SE of the test accuracy across the folds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2KF4V4bbkPU",
        "outputId": "a210a978-e700-45b1-e42d-59a2c264b267"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# TODO 13\n",
        "nfold = 10\n",
        "kf = KFold(n_splits=nfold,shuffle=True,random_state=3)\n",
        "# Initial metrics array\n",
        "C = np.zeros((len(str_list),len(str_list)))\n",
        "acc = np.zeros(nfold)\n",
        "\n",
        "for i, I in enumerate(kf.split(X)):\n",
        "    \n",
        "    # Get training and test data\n",
        "    train, test = I\n",
        "\n",
        "    Xtr = X.values[train,:]\n",
        "    ytr = y[train]\n",
        "    Xts = X.values[test,:]\n",
        "    yts = y[test]\n",
        "    \n",
        "    # Scale the data\n",
        "    scal = StandardScaler()\n",
        "    Xtr1 = scal.fit_transform(Xtr)\n",
        "    Xts1 = scal.transform(Xts)    \n",
        "    \n",
        "    # Fit a model\n",
        "    reg=linear_model.LogisticRegression(C=1e5,solver='liblinear')    \n",
        "    reg.fit(Xtr1, ytr)\n",
        "    \n",
        "    # Predict on test samples and measure accuracy\n",
        "    yhat = reg.predict(Xts1)\n",
        "    C = C+confusion_matrix(yts,yhat)\n",
        "    acc[i]=np.mean(yhat == yts)\n",
        "\n",
        "C_all=C/np.sum(C,axis=1)\n",
        "print(np.array_str(C_all, precision=4, suppress_small=True))\n",
        "accm= np.mean(acc)\n",
        "acc_se = np.std(acc)/np.sqrt(nfold-1)\n",
        "print('Accuracy =  {0:.4f}, SE={1:.4f}'.format(accm, acc_se))\n",
        "    "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.9733 0.0148 0.     0.     0.     0.019  0.     0.    ]\n",
            " [0.0267 0.9556 0.     0.     0.0074 0.     0.0074 0.    ]\n",
            " [0.     0.0074 0.9867 0.0074 0.     0.     0.     0.    ]\n",
            " [0.0067 0.     0.     0.9852 0.0074 0.     0.     0.    ]\n",
            " [0.     0.0074 0.     0.     0.9926 0.     0.     0.    ]\n",
            " [0.0067 0.     0.     0.     0.     0.9905 0.     0.    ]\n",
            " [0.     0.     0.     0.0074 0.     0.     0.9926 0.    ]\n",
            " [0.     0.     0.     0.     0.     0.     0.     1.    ]]\n",
            "Accuracy =  0.9843, SE=0.0044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCzoWjETIoAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec873328-c234-4190-d5fc-63ea506634b2"
      },
      "source": [
        "C/np.sum(C,axis=1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.97333333, 0.01481481, 0.        , 0.        , 0.        ,\n",
              "        0.01904762, 0.        , 0.        ],\n",
              "       [0.02666667, 0.95555556, 0.        , 0.        , 0.00740741,\n",
              "        0.        , 0.00740741, 0.        ],\n",
              "       [0.        , 0.00740741, 0.98666667, 0.00740741, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.00666667, 0.        , 0.        , 0.98518519, 0.00740741,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.00740741, 0.        , 0.        , 0.99259259,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.00666667, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.99047619, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.00740741, 0.        ,\n",
              "        0.        , 0.99259259, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px-wMi0kbkPU"
      },
      "source": [
        "Re-run the logistic regression on the entire training data and get the weight coefficients.  This should be a 8 x 77 matrix.  Create a stem plot of the first row of this matrix to see the coefficients on each of the genes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "cl3rtcWGbkPU",
        "outputId": "76da906f-9fc6-4ef6-a758-abae0cec37d6"
      },
      "source": [
        "# TODO 14\n",
        "reg=linear_model.LogisticRegression(C=1e5,solver='liblinear')    \n",
        "reg.fit(X,y)\n",
        "w=reg.coef_\n",
        "w1=w[0,:]\n",
        "plt.stem(w1,use_line_collection=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<StemContainer object of 3 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "HFEDtEU4bkPU"
      },
      "source": [
        "## L1-Regularization\n",
        "\n",
        "This section is bonus.\n",
        "\n",
        "In most genetic problems, only a limited number of the tested genes are likely influence any particular attribute.  Hence, we would expect that the weight coefficients in the logistic regression model should be sparse.  That is, they should be zero on any gene that plays no role in the particular attribute of interest.  Genetic analysis commonly imposes sparsity by adding an l1-penalty term.  Read the `sklearn` [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) on the `LogisticRegression` class to see how to set the l1-penalty and the inverse regularization strength, `C`.\n",
        "\n",
        "Using the model selection strategies from the [housing demo](../unit05_lasso/demo2_housing.ipynb), use K-fold cross validation to select an appropriate inverse regularization strength.  \n",
        "* Use 10-fold cross validation \n",
        "* You should select around 20 values of `C`.  It is up to you find a good range.\n",
        "* Make appropriate plots and print out to display your results\n",
        "* How does the accuracy compare to the accuracy achieved without regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLLnOLOebkPU"
      },
      "source": [
        "# TODO 15"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "wVniO2ENbkPV"
      },
      "source": [
        "# Create cross-validation object\n",
        "nfold = 10\n",
        "kf =KFold(nfold, shuffle=True)\n",
        "\n",
        "# Alpha values to test\n",
        "C_list = np.logspace(0,3,20)\n",
        "nC = len(C_list)\n",
        "acc = np.zeros((nC, nfold))\n",
        "acc_noreg = np.zeros((nC, nfold))\n",
        "# Run the cross-validation\n",
        "for ifold, I in enumerate(kf.split(X)):\n",
        "    \n",
        "    # Get training and test data\n",
        "    train, test = I\n",
        "\n",
        "    Xtr = X.values[train,:]\n",
        "    ytr = y[train]\n",
        "    Xts = X.values[test,:]\n",
        "    yts = y[test]\n",
        "    \n",
        "    # Scale the data\n",
        "    scal = StandardScaler()\n",
        "    Xtr1 = scal.fit_transform(Xtr)\n",
        "    Xts1 = scal.transform(Xts)    \n",
        "    \n",
        "\n",
        "    for i, C in enumerate(C_list):\n",
        "        # Fit a model\n",
        "        reg=linear_model.LogisticRegression(penalty='l1',C=C,solver='liblinear')    \n",
        "        reg.fit(Xtr1, ytr)\n",
        "\n",
        "        reg_noreg=linear_model.LogisticRegression(penalty='none')    \n",
        "        reg_noreg.fit(Xtr1, ytr)\n",
        "\n",
        "        \n",
        "        # Predict on test samples and measure accuracy\n",
        "        yhat = reg.predict(Xts1)\n",
        "        yhat2 = reg_noreg.predict(Xts1)\n",
        "\n",
        "        # Score on the test data\n",
        "        acc[i, ifold] = np.mean(yhat == yts)\n",
        "        acc_noreg[i, ifold] = np.mean(yhat2 == yts)\n",
        "    \n",
        "        # print('Fold = %d, C= %d' % (ifold,C))"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxvKIrZxDzqG",
        "outputId": "54e9cbe0-ec69-4b45-d2e8-eae906683298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "# plot C and acc\n",
        "acc_mean = np.mean(acc, axis=1)\n",
        "acc_se  = np.std(acc, axis=1) / np.sqrt(nfold-1)\n",
        "plt.plot(C_list,acc_mean)\n",
        "plt.grid()\n",
        "plt.xlabel('C')\n",
        "plt.ylabel('acc')\n",
        "idx=np.argmax(acc_mean)\n",
        "best_C=C_list[idx]\n",
        "\n",
        "# compare with no reg\n",
        "acc_noreg_mean = np.mean(acc_noreg[1,:])\n",
        "diff = acc_mean[idx] - acc_noreg_mean\n",
        "\n",
        "print('The best C for l1 is %.4f, mean acc is %.4f, %.4f better than model without regularization.' % (best_C,acc_mean[idx],diff))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best C for l1 is 2.9764, mean acc is 0.9944, 0.0046 better than model without regularization.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}